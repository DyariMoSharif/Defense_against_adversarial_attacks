{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d6c9610-0384-4c66-a603-37d45944b29f",
   "metadata": {},
   "source": [
    "### Display the versions of the libraries used for reference purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5ac8ce8-2787-43f2-8e8d-688e73829e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version: 3.12.9 | packaged by Anaconda, Inc. | (main, Feb  6 2025, 18:49:16) [MSC v.1929 64 bit (AMD64)]\n",
      "Jupyter Notebook version: 7.3.2\n",
      "NumPy version: 1.26.4\n",
      "TensorFlow version: 2.19.0\n",
      "Torch version: 2.6.0+cu126\n",
      "Scikit-learn version: 1.5.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import torch\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import notebook\n",
    "import os\n",
    "\n",
    "# Print Python version\n",
    "print(f'Python version: {sys.version}')\n",
    "\n",
    "# Print Jupyter Notebook version\n",
    "print(f'Jupyter Notebook version: {notebook.__version__}')\n",
    "\n",
    "# Print library versions\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'Torch version: {torch.__version__}')\n",
    "print(f'Scikit-learn version: {sklearn.__version__}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0818632f-2d13-41bc-92c0-e16a41d81dd9",
   "metadata": {},
   "source": [
    "### Loading MNIST dataset and Splitting its and saving them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0e2a8f-8493-4097-8338-53f4fbfae969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Models and Data splits/data_[SCALED] Train_Test_Splits.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "#Create Models and Data splits Folder\n",
    "os.makedirs(\"Models and Data splits\", exist_ok=True)\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# 1. Load and Preprocess the MNIST Data\n",
    "# -------------------------\n",
    "mnist = fetch_openml('mnist_784', version=1)\n",
    "X = mnist.data.values      # shape (70000, 784)\n",
    "y = mnist.target.astype(int).values\n",
    "\n",
    "# 2. Split into train/test\n",
    "X_train_not_scaled, X_test_not_scaled, \\\n",
    "y_train_not_scaled, y_test_not_scaled = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "# 3. Save the raw (unscaled) splits\n",
    "joblib.dump(\n",
    "    (X_train_not_scaled, X_test_not_scaled,\n",
    "     y_train_not_scaled, y_test_not_scaled),\n",
    "    'Models and Data splits/data_[ORIGINAL] Train_Test_Splits.pkl'\n",
    ")\n",
    "\n",
    "# 4. [0,1] Min‚ÄìMax scaling (divide by 255)\n",
    "X_train_scaled = X_train_not_scaled / 255.0\n",
    "X_test_scaled  = X_test_not_scaled  / 255.0\n",
    "joblib.dump(\n",
    "    (X_train_scaled, X_test_scaled,\n",
    "     y_train_not_scaled, y_test_not_scaled),\n",
    "    'Models and Data splits/data_[SCALED] Train_Test_Splits.pkl'\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cbd11b4-7104-4599-8ceb-df15d76d900b",
   "metadata": {},
   "source": [
    "### Trainning leNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a38e34-2f6c-4c12-8fd4-56a7d5a1a437",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1/100, Loss: 1.8500\n",
      "Epoch 2/100, Loss: 0.6422\n",
      "Epoch 3/100, Loss: 0.4237\n",
      "Epoch 4/100, Loss: 0.3457\n",
      "Epoch 5/100, Loss: 0.2980\n",
      "Epoch 6/100, Loss: 0.2602\n",
      "Epoch 7/100, Loss: 0.2279\n",
      "Epoch 8/100, Loss: 0.2005\n",
      "Epoch 9/100, Loss: 0.1776\n",
      "Epoch 10/100, Loss: 0.1586\n",
      "Epoch 11/100, Loss: 0.1428\n",
      "Epoch 12/100, Loss: 0.1299\n",
      "Epoch 13/100, Loss: 0.1192\n",
      "Epoch 14/100, Loss: 0.1101\n",
      "Epoch 15/100, Loss: 0.1024\n",
      "Epoch 16/100, Loss: 0.0957\n",
      "Epoch 17/100, Loss: 0.0898\n",
      "Epoch 18/100, Loss: 0.0850\n",
      "Epoch 19/100, Loss: 0.0805\n",
      "Epoch 20/100, Loss: 0.0768\n",
      "Epoch 21/100, Loss: 0.0731\n",
      "Epoch 22/100, Loss: 0.0700\n",
      "Epoch 23/100, Loss: 0.0673\n",
      "Epoch 24/100, Loss: 0.0645\n",
      "Epoch 25/100, Loss: 0.0621\n",
      "Epoch 26/100, Loss: 0.0601\n",
      "Epoch 27/100, Loss: 0.0580\n",
      "Epoch 28/100, Loss: 0.0561\n",
      "Epoch 29/100, Loss: 0.0545\n",
      "Epoch 30/100, Loss: 0.0527\n",
      "Epoch 31/100, Loss: 0.0513\n",
      "Epoch 32/100, Loss: 0.0498\n",
      "Epoch 33/100, Loss: 0.0483\n",
      "Epoch 34/100, Loss: 0.0472\n",
      "Epoch 35/100, Loss: 0.0461\n",
      "Epoch 36/100, Loss: 0.0447\n",
      "Epoch 37/100, Loss: 0.0439\n",
      "Epoch 38/100, Loss: 0.0427\n",
      "Epoch 39/100, Loss: 0.0417\n",
      "Epoch 40/100, Loss: 0.0408\n",
      "Epoch 41/100, Loss: 0.0400\n",
      "Epoch 42/100, Loss: 0.0392\n",
      "Epoch 43/100, Loss: 0.0382\n",
      "Epoch 44/100, Loss: 0.0375\n",
      "Epoch 45/100, Loss: 0.0367\n",
      "Epoch 46/100, Loss: 0.0359\n",
      "Epoch 47/100, Loss: 0.0353\n",
      "Epoch 48/100, Loss: 0.0346\n",
      "Epoch 49/100, Loss: 0.0340\n",
      "Epoch 50/100, Loss: 0.0333\n",
      "Epoch 51/100, Loss: 0.0328\n",
      "Epoch 52/100, Loss: 0.0322\n",
      "Epoch 53/100, Loss: 0.0317\n",
      "Epoch 54/100, Loss: 0.0311\n",
      "Epoch 55/100, Loss: 0.0305\n",
      "Epoch 56/100, Loss: 0.0301\n",
      "Epoch 57/100, Loss: 0.0296\n",
      "Epoch 58/100, Loss: 0.0291\n",
      "Epoch 59/100, Loss: 0.0286\n",
      "Epoch 60/100, Loss: 0.0279\n",
      "Epoch 61/100, Loss: 0.0277\n",
      "Epoch 62/100, Loss: 0.0273\n",
      "Epoch 63/100, Loss: 0.0267\n",
      "Epoch 64/100, Loss: 0.0265\n",
      "Epoch 65/100, Loss: 0.0261\n",
      "Epoch 66/100, Loss: 0.0256\n",
      "Epoch 67/100, Loss: 0.0252\n",
      "Epoch 68/100, Loss: 0.0248\n",
      "Epoch 69/100, Loss: 0.0246\n",
      "Epoch 70/100, Loss: 0.0242\n",
      "Epoch 71/100, Loss: 0.0238\n",
      "Epoch 72/100, Loss: 0.0234\n",
      "Epoch 73/100, Loss: 0.0231\n",
      "Epoch 74/100, Loss: 0.0227\n",
      "Epoch 75/100, Loss: 0.0224\n",
      "Epoch 76/100, Loss: 0.0222\n",
      "Epoch 77/100, Loss: 0.0219\n",
      "Epoch 78/100, Loss: 0.0215\n",
      "Epoch 79/100, Loss: 0.0212\n",
      "Epoch 80/100, Loss: 0.0209\n",
      "Epoch 81/100, Loss: 0.0206\n",
      "Epoch 82/100, Loss: 0.0203\n",
      "Epoch 83/100, Loss: 0.0200\n",
      "Epoch 84/100, Loss: 0.0198\n",
      "Epoch 85/100, Loss: 0.0196\n",
      "Epoch 86/100, Loss: 0.0192\n",
      "Epoch 87/100, Loss: 0.0190\n",
      "Epoch 88/100, Loss: 0.0188\n",
      "Epoch 89/100, Loss: 0.0185\n",
      "Epoch 90/100, Loss: 0.0183\n",
      "Epoch 91/100, Loss: 0.0182\n",
      "Epoch 92/100, Loss: 0.0179\n",
      "Epoch 93/100, Loss: 0.0176\n",
      "Epoch 94/100, Loss: 0.0174\n",
      "Epoch 95/100, Loss: 0.0172\n",
      "Epoch 96/100, Loss: 0.0169\n",
      "Epoch 97/100, Loss: 0.0167\n",
      "Epoch 98/100, Loss: 0.0164\n",
      "Epoch 99/100, Loss: 0.0163\n",
      "Epoch 100/100, Loss: 0.0162\n",
      "\n",
      "Test Accuracy: 0.9896\n",
      "Model saved to Models/lenet.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "\n",
    "# 1. Define LeNet architecture with Tanh activations and average pooling\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5)   # 28x28 -> 24x24\n",
    "        self.pool = nn.AvgPool2d(kernel_size=2, stride=2)  # 24x24 -> 12x12\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5)  # 12x12 -> 8x8\n",
    "        # Pool again 8x8 -> 4x4\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.tanh(self.conv1(x)))\n",
    "        x = self.pool(torch.tanh(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = torch.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# 2. Load your data\n",
    "X_train, X_test, y_train, y_test = joblib.load('Models and Data splits/data_[SCALED] Train_Test_Splits.pkl')\n",
    "\n",
    "# 3. Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# 4. Prepare datasets and dataloaders\n",
    "batch_size = 64\n",
    "\n",
    "# Convert numpy arrays to torch tensors and reshape for CNN input (N,1,28,28)\n",
    "X_train_tensor = torch.tensor(X_train.reshape(-1,1,28,28), dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test.reshape(-1,1,28,28), dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 5. Initialize model, loss, optimizer\n",
    "model = LeNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # SGD optimizer as original paper\n",
    "\n",
    "# 6. Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# 7. Evaluation on test set\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "        all_preds.extend(preds)\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"\\nTest Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# 8. Save trained model\n",
    "\n",
    "# Save the scripted model\n",
    "model = torch.jit.script(model)\n",
    "torch.jit.save(model, \"Models and Data splits/lenet.pt\")\n",
    "print(\"Model saved to Models/lenet.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76f8b69-2a99-45c6-b0f0-1aca8fed4128",
   "metadata": {},
   "source": [
    "### Correctly classified samples (100 per each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "732d4e4c-cc40-404e-994e-3499b7224950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting sample selection process...\n",
      "Using device: cuda\n",
      "‚úÖ Data scaled to [0, 1] (min=0.0000, max=1.0000)\n",
      "\n",
      "üîé Searching for 100 correctly classified samples per digit...\n",
      "   Found enough samples for all digits.\n",
      "   Digit 0: Found 100 samples.\n",
      "   Digit 1: Found 100 samples.\n",
      "   Digit 2: Found 100 samples.\n",
      "   Digit 3: Found 100 samples.\n",
      "   Digit 4: Found 100 samples.\n",
      "   Digit 5: Found 100 samples.\n",
      "   Digit 6: Found 100 samples.\n",
      "   Digit 7: Found 100 samples.\n",
      "   Digit 8: Found 100 samples.\n",
      "   Digit 9: Found 100 samples.\n",
      "\n",
      "‚úÖ Successfully saved 1000 samples to:\n",
      "   C:\\Users\\dyari\\OneDrive\\Desktop\\RF_Verification_Layer_Defense\\Models and Data splits\\selected_samples_for_attack.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import joblib\n",
    "from collections import defaultdict\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import os\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = \"Models and Data splits/lenet.pt\"\n",
    "DATA_PATH = \"Models and Data splits/data_[ORIGINAL] Train_Test_Splits.pkl\"\n",
    "OUTPUT_PATH = \"Models and Data splits/selected_samples_for_attack.pt\"\n",
    "SAMPLES_PER_DIGIT = 100\n",
    "\n",
    "# --- 1. Load Model and Data ---\n",
    "print(\"üöÄ Starting sample selection process...\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load the trained model\n",
    "try:\n",
    "    model = torch.jit.load(MODEL_PATH).to(device).eval()\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Load the test data\n",
    "try:\n",
    "    _, X_test, _, y_test = joblib.load(DATA_PATH)\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: Data file not found at '{DATA_PATH}'.\")\n",
    "    exit()\n",
    "\n",
    "# Reshape and scale data to [0, 1]\n",
    "X_test = X_test.reshape(-1, 1, 28, 28).astype(\"float32\") / 255.0\n",
    "\n",
    "# Confirm scaling\n",
    "if X_test.min() < 0 or X_test.max() > 1:\n",
    "    print(f\"‚ö†Ô∏è Warning: Data is not properly scaled (min={X_test.min():.4f}, max={X_test.max():.4f}).\")\n",
    "else:\n",
    "    print(f\"‚úÖ Data scaled to [0, 1] (min={X_test.min():.4f}, max={X_test.max():.4f})\")\n",
    "\n",
    "# --- 2. Find Correctly Classified Samples ---\n",
    "print(f\"\\nüîé Searching for {SAMPLES_PER_DIGIT} correctly classified samples per digit...\")\n",
    "\n",
    "# Convert the entire test set to tensors for efficient processing\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.int64).to(device)\n",
    "\n",
    "final_indices_per_digit = defaultdict(list)\n",
    "batch_size = 500  # Process in batches for memory efficiency\n",
    "\n",
    "# Create a DataLoader for the entire test set\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor, torch.arange(len(y_test_tensor)))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images_batch, labels_batch, indices_batch in test_loader:\n",
    "        # Stop searching if we have found enough samples for all digits\n",
    "        if len(final_indices_per_digit) == 10 and all(len(v) >= SAMPLES_PER_DIGIT for v in final_indices_per_digit.values()):\n",
    "            print(\"   Found enough samples for all digits.\")\n",
    "            break\n",
    "\n",
    "        outputs = model(images_batch)\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        correct_mask = (preds == labels_batch)\n",
    "        \n",
    "        # Iterate over the correctly predicted samples within this batch\n",
    "        for i in torch.where(correct_mask)[0]:\n",
    "            label = labels_batch[i].item()\n",
    "            if len(final_indices_per_digit[label]) < SAMPLES_PER_DIGIT:\n",
    "                original_index = indices_batch[i].item()\n",
    "                final_indices_per_digit[label].append(original_index)\n",
    "\n",
    "# --- 3. Create and Save the Final Dataset ---\n",
    "\n",
    "# Trim any excess samples and gather the final indices\n",
    "final_indices = []\n",
    "for digit in sorted(final_indices_per_digit.keys()):\n",
    "    indices = final_indices_per_digit[digit][:SAMPLES_PER_DIGIT]\n",
    "    final_indices.extend(indices)\n",
    "    print(f\"   Digit {digit}: Found {len(indices)} samples.\")\n",
    "\n",
    "if len(final_indices) != SAMPLES_PER_DIGIT * 10:\n",
    "     print(f\"\\n‚ö†Ô∏è Warning: Could not find {SAMPLES_PER_DIGIT} samples for every digit.\")\n",
    "     print(\"   The resulting file will contain fewer than 1000 samples.\")\n",
    "\n",
    "# Select the final images and labels using the collected indices\n",
    "final_images = X_test_tensor[final_indices]\n",
    "final_labels = y_test_tensor[final_indices]\n",
    "\n",
    "# Save the tensors to a file\n",
    "saved_data = {\n",
    "    'images': final_images.cpu(),  # Save on CPU to avoid device issues when loading\n",
    "    'labels': final_labels.cpu()\n",
    "}\n",
    "torch.save(saved_data, OUTPUT_PATH)\n",
    "\n",
    "print(f\"\\n‚úÖ Successfully saved {len(final_images)} samples to:\")\n",
    "print(f\"   {os.path.abspath(OUTPUT_PATH)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:GPUEnabled]",
   "language": "python",
   "name": "conda-env-GPUEnabled-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
